---
title: "Laporan Responsi II"
author: "Hana Faiza Amalina"
date: "`r Sys.Date()`"
output: html_document
---

## Pendahuluan
### Materi
_Statistical learning_ merupakan pembelajaran statistik yang mengacu pada seperangkat alat yang luas untuk memahami data. Alat-alat tersebut dapat diklasifikasikan sebagai **_supervised learning_** dan **_unsupervised learning_**. _Supervised learning_ melibatkan pembangunan model untuk memprediksi peubah respon Y berdasarkan peubah penjelas X atau mencari hubungan antara peubah penjelas X dengan peubah respon Y, sedangkan _unsupervised learning_ digunakan untuk mengelompokkan objek berdasarkan kesamaan karakteristik dari peubah penjelas X. 

### Data
#### _Supervised Learning_
Data yang digunakan pada _supervised learning_ adalah dataset ```abalone``` dari ```library(AppliedPredictiveModeling)```. Data ini terdiri atas 4177 amatan dengan sembilan peubah, yaitu:

1. ```Sex```: Jenis kelamin abalone (M, F, dan I (infant atau bayi))

2. ```Length```: Panjang abalone

3. ```Diameter```: Diameter abalone

4. ```Height```: Tinggi abalone

5. ```Whole weight```: Berat keseluruhan abalone

6. ```Shucked weight```: Berat abalone terkupas

7. ```Viscera weight```: Berat jeroan abalone

8. ```Shell weight```: Berat cangkang abalone

9. ```Rings```: Jumlah cincin pada abalone (menunjukkan usia)

Peubah ```Rings``` merupakan peubah respon, sedangkan kedelapan peubah lainnya berperan sebagai peubah penjelas. Tujuan penggunaan data ini adalah memprediksi usia abalone berdasarkan ukurannya.

```{r}
library("AppliedPredictiveModeling")
data(abalone)
head(abalone)
```

#### _Unsupervised Learning_
Data yang digunakan pada _unsupervised learning_ adalah dataset ```Glass``` dari ```library(mlbench)```. Data ini terdiri atas 214 amatan dengan sepuluh peubah, yaitu:

1. RI (Refractive Index): Indeks bias

2. Na: Sodium (Natrium) 

3. Mg: Magnesium

4. Al: Alumunium

5. Si: Silikon

6. K: Kalium

7. Ca: Kalsium

8. Ba: Barium

9. Fe: Besi

10. Type: Tipe gelas (1: Jendela bangunan diproses terapung; 2: Jendela bangunan tidak diproses terapung; 3: Jendela kendaraan diproses terapung; 4: Jendela kendaraan tidak diproses terapung; 5: Wadah; 6: Peralatan makan; 7: Lampu depan)

Tujuan penggunakaan ini pada _unsupervised learning_ adalah mencoba menemukan pola komposisi kimia suatu jenis kaca melalui analisis gerombol. Gerombol yang dibentuk mungkin dapat digunakan untuk menarik kesimpulan tentang kaca.

```{r}
library(mlbench)
data("Glass")
head(Glass)
```

### Library
Library R yang digunakan adalah sebagai berikut:

1. ```dplyr```

2. ```corrplot```

3. ```glmnet```

4. ```ggplot2```

5. ```plyr```

6. ```tidyr```

7. ```tidyverse```

8. ```caret```

9. ```MuMIn```

10. ```broom```

11. ```MASS```

12. ```leaps```

13. ```factoextra```

14. ```Hmisc```

```{r message=FALSE, warning=FALSE, include=FALSE}
defaultW <- getOption("warn") 
options(warn = -1)
library(dplyr)
library(corrplot)
library(glmnet)
library(ggplot2)
library(MuMIn)
library(broom)
library(MASS)
library(leaps)
library(factoextra)
library(plyr)
library(tidyr)
library(tidyverse)
library(caret)
library(Hmisc)
options(warn = defaultW)
```

### Pembentukan Fungsi Evaluasi Model
Model-model yang dibentuk nantinya akan dievaluasi menggunakan RMSE dan R-Square sebagai ukuran keakuratan. Model yang baik adalah model dengan RMSE rendah dan R-Square tinggi.

```{r}
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - (SSE/SST)
  RMSE = sqrt(SSE/nrow(df))
  data.frame(RMSE = RMSE, Rsquare = R_square)
}
```

## Preprocessing Data
### Data Abalone
```{r}
summary(abalone)
```

#### Cek Duplikasi Data
```{r}
abalone[duplicated(abalone),]
```

Tidak ada data yang duplikat pada dataset abalone.

#### Cek Missing Value(s) (NA)
```{r}
abalone[!complete.cases(abalone),]
which(is.na(abalone), arr.ind=TRUE)
```

Tidak ada missing value pada dataset abalone.

#### Height = 0
Berdasarkan ringkasan data, nilai minimum peubah ```Height``` adalah nol, maka perlu diperiksa berapa total baris dengan  ```Height = 0```

```{r}
abalone[abalone$Height==0,]
nrow(abalone[abalone$Height==0,])
```

#### Merapikan Dataset
Terakhir, dataset dirapikan dengan langkah-langkah sebagai berikut:

+ Menghapus spasi di depan dan di belakang pada kolom dengan data jenis karakter

+ Menambahkan kolom yang berisi selisih berat (sebelum dan sesudah abalone diproses)

+ Hapus baris dengan ```Height = 0```

+ Hapus baris dengan Weight.diff <= 0 karena berat keseluruhan setiap abalon harus lebih besar dari total sub bagiannya (dikupas, jeroan dan cangkang)

+ Hapus kolom Weight.diff

```{r}
df <- abalone %>%
  mutate_if(is.character, str_trim) %>%
  mutate(Weight.diff = WholeWeight - (VisceraWeight + ShuckedWeight + ShellWeight)) %>%
  subset(Height > 0) %>%
  subset(Weight.diff > 0) 

df <- df[,-10]

names(df) <- c("Sex", "Length", "Diameter", "Height", "Whole", "Shucked", "Viscera", "Shell", "Rings")

df$Sex <- ordered(df$Sex, 
                  levels = c("I", "M", "F"), 
                  labels = c("Infant", "Male", "Female"))
```

Berikut ini merupakan data akhir setelah dilakukan preprocessing data. Selanjutnya, data ini yang akan digunakan dalam analisis.

```{r}
head(df)
```

### Data Glass
```{r}
summary(Glass)
```

#### Cek Duplikasi Data
```{r}
Glass[duplicated(Glass),]
```

Hapus data yang duplikat.

```{r}
dt <- Glass[-40,]
dt[duplicated(dt),]
```



#### Cek Missing Value(s) (NA)
```{r}
dt[!complete.cases(dt),]
```

Tidak ada missing value pada dataset glass.

#### Merapikan Dataset
Dataset ```Glass``` dirapikan dengan langkah-langkah sebagai berikut:

+ Menghilangkan peubah ```RI``` dan ```Type``` karena tidak ada informasi yang bisa diambil dari sana.

+ Melakukan standardisasi pada peubah yang tersisa karena setiap peubah memiliki skala yang berbeda. Penggerombolan data erat kaitannya dengan jarak, sehingga perbedaan skala dapat mempengaruhi hasil penggerombolan. Standardisasi dilakukan menggunakan z-score.

```{r}
dt <- dt[,-c(1,10)]
scale <- function(x) (x-mean(x, na.rm = TRUE))/ sd(x, na.rm = TRUE)
dt.scl <- dt %>%
  mutate_if(is.numeric, scale)
```

Berikut ini merupakan data akhir setelah dilakukan preprocessing data. Selanjutnya, data ini yang akan digunakan dalam analisis.

```{r}
head(dt.scl)
```

## Eksplorasi Data
Eksplorasi data penting untuk dilakukan pada awal analisis untuk mengenal struktur data itu sendiri. 

### Data Abalone
#### Plot Korelasi
```{r}
corr<-cor(df[c(-1,-10)], method = "pearson", use = "complete.obs")
round(corr, 2)
corrplot(corr, type = 'lower', order = 'hclust', tl.col = 'black',
         tl.srt = 45, tl.cex=0.8,  addCoef.col = 'black', number.cex=0.8, col = COL2('RdYlBu'), cl.pos='n')
```

Plot korelasi di atas menunjukkan hubungan yang positif antar peubahnya. Warna biru yang semakin pekat menunjukkan hubungan yang kuat, begitu pula sebaliknya.

#### Sebaran Jenis Kelamin
```{r}
ggplot(data = df,
       aes(x = Sex, fill = Sex)) +
  geom_bar()
```

#### Hubungan Antar Jenis Berat Berdasarkan Jenis Kelamin
##### Whole vs Shucked
```{r}
qplot(Whole, Shucked, data = df, color = Sex)
```

##### Whole vs Viscera
```{r}
qplot(Whole, Viscera, data = df, color = Sex)
```

##### Whole vs Shell
```{r}
qplot(Whole, Shell, data = df, color = Sex)
```

### Data Glass
#### Plot Korelasi

```{r}
c <- cor(dt, method = "pearson", use = "complete.obs")
round(corr, 2)
corrplot(c, type = 'lower', order = 'hclust', tl.col = 'black',
         tl.srt = 45, tl.cex=0.8,  addCoef.col = 'black', number.cex=0.8, col = COL2('RdYlBu'), cl.pos='n')
```

Plot korelasi di atas menunjukkan hubungan relatif rendah antar peubahnya. Warna yang semakin pekat menunjukkan hubungan yang kuat, begitu pula sebaliknya.

#### Sebaran Seluruh Peubah

```{r}
hist.data.frame(dt)
```

Berdasarkan ringkasan plot histogram di atas, dapat dideteksi kemungkinan adanya pencilan. Misalnya, nilai maksimum untuk ```K```, ```Ba```, dan ```Fe``` jauh lebih tinggi daripada kuantik ke-3. 

##### Histogram K
```{r}
hist(dt$K)
```

Terdapat pencilan pada peubah ini, sehingga amatan tersebut akan dihapus karena dapat menimbulkan masalah pada penggerombolan.

```{r}
dt <- dt[dt$K<6,]
```

##### Histogram Ba
```{r}
hist(dt$Ba)
```
Pada peubah ```Ba```, sebagian besar amatan memiliki nilai Ba = 0. Maka dari itu, peubah ini akan dihapus.

```{r}
dt <- dt[,-7]
```

##### Histogram Fe
```{r}
hist(dt$Fe)
```

Peubah ```Fe``` juga memiliki banyak amatan dengan nilai Fe = 0. Maka peubah ini juga akan dihapus

```{r}
dt <- dt[,-7]
```

Berdasarkan hasil preprocessing dan eksplorasi data, berikut data final yang akan digunakan dalam analisis gerombol.
```{r}
head(dt)
```

## Supervised Learning
### Partisi data
Data dibagi menjadi dua, yaitu data training (data latih) dan data testing (data uji). Data training digunakan untuk membangun model, sedangkan data testing digunakan untuk memvalidasi model yang sudah dibangun. 

```{r}
set.seed(100) 

index <- sample(1:nrow(df), 0.8*nrow(df)) 

train = df[index,] 
test = df[-index,]

dim(train)
dim(test)
```

Dengan menggunakan proporsi 80:20, 3212 amatan akan digunakan sebagai data training, sedangkan 804 amatan sisanya akan digunakan sebagai data testing.

### Ridge Regression
Ide dari ridge regression adalah emminimalkan jumlah kuadrat galat yang terikat pada regularisasi L_2 dari koefisiennya. Ridge regression digunakan pada pemodelan regresi dengan peubah penjelas untuk menduga koefisien regresi dengan peubah penjelas  yang saling berkorelasi (multikolinieritas tinggi). Sifat dugaan parameter yang dihasilkan oleh ridge regression adalah berbias dengan mempertahankan semua peubah penjelasnya.

```{r warning=FALSE}
lambdas <- 10^seq(2, -3, by = -.1)
ridge_reg <- glmnet(as.matrix(train[,-c(1,9)]), train[,9], alpha = 0, 
                   family = 'gaussian', lambda = lambdas)
summary(ridge_reg)
```
```{r}
ridge_reg
```

Hasil di atas menunjukkan perbandingan antar nilai lambda yang didefinisikan sehingga bebas untuk menentukan lambda mana yang akan dipilih. Lambda optimal akan dipilih menggunakan cross validation (cv). Semakin kecil nilai lambda yang dipilih, maka modelnya akan menjadi semakin kompleks.

```{r}
set.seed(100)
cv_ridge <- cv.glmnet(as.matrix(train[,-c(1,9)]), train[,9], alpha = 0, lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
```

Nilai lambda optimal yang diperoleh untuk model ini adalah 0.001. Nilai parameter tersebut akan digunakan untuk menduga koefisien dari peubah-peubah penjelas pada model ridge regression.

```{r}
coef(cv_ridge, s = "lambda.min")
```

Selanjutnya akan dilakukan prediksi dan evaluasi pada data training dan data testing.

```{r}
predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = as.matrix(train[,-c(1,9)]))
eval_results(train[,9], predictions_train, train)
```

Nilai R-square yang diperoleh pada prediksi data testing adalah 0.5208. Artinya, sebanyak 52.08% keragaman model mampu dijelaskan oleh peubah penjelas yang ada dalam model, sisanya dijelaskan oleh peubah lain.

```{r}
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = as.matrix(test[,-c(1,9)]))
eval_results(test[,9], predictions_test, test)
```

Terdapat kenaikan nilai RMSE dan R-Square pada prediksi menggunakan data testing. Nilai R-square yang diperoleh kali ini adalah 0.5390. Artinya, sebanyak 53.9% keragaman model mampu dijelaskan oleh peubah penjelas yang ada dalam model, sisanya dijelaskan oleh peubah lain.

### Lasso Regression
Ide dari lasso regression adalah melakukan penyusutan berkelanjutan (shrinkage) dan pemilihan variabel otomatis secara bersamaan. Lasso regression digunakan pada pemodelan regresi dengan peubah penjelas sangat banyak untuk menyeleksi peubah penjelas dalam model. Sifat dugaan parameter yang dihasilkan oleh lasso regression adalah berbias, namun tidak memungkinkan semua peubah penjelas terpilih dalam model.

Sama seperti model ridge regression, model lasso regression juga memerlukan suatu parameter lambda. Maka dari itu, perlu dilakukan penentuan lambda optimal menggunakan cross validation. 

```{r}
cv_lasso <- cv.glmnet(as.matrix(train[,-c(1,9)]), train[,9], alpha = 1, 
                      lambda = lambdas, standardize = TRUE)
best_lambda <- cv_lasso$lambda.min 
best_lambda
```

Nilai lambda optimal yang diperoleh untuk model ini adalah 0.001. Nilai parameter tersebut akan digunakan untuk menduga koefisien dari peubah-peubah penjelas pada model lasso regression.

```{r}
lasso_reg <- glmnet(as.matrix(train[,-c(1,9)]), train[,9], alpha = 1, 
                      lambda = best_lambda, standardize = TRUE)
coef(cv_lasso, s = "lambda.min")
```

Selanjutnya akan dilakukan prediksi dan evaluasi pada data training dan data testing.

```{r}
predictions_train <- predict(lasso_reg, s = best_lambda, newx = as.matrix(train[,-c(1,9)]))
eval_results(train[,9], predictions_train, train)
```

Nilai R-square yang diperoleh pada prediksi data testing adalah 0.5209. Artinya, sebanyak 52.09% keragaman model mampu dijelaskan oleh peubah penjelas yang ada dalam model, sisanya dijelaskan oleh peubah lain.

```{r}
predictions_test <- predict(lasso_reg, s = best_lambda, newx = as.matrix(test[,-c(1,9)]))
eval_results(test[,9], predictions_test, test)
```

Terdapat kenaikan nilai RMSE dan R-Square pada prediksi menggunakan data testing. Nilai R-square yang diperoleh kali ini adalah 0.5393. Artinya, sebanyak 53.93% keragaman model mampu dijelaskan oleh peubah penjelas yang ada dalam model, sisanya dijelaskan oleh peubah lain.

### Model Averaging
Model averaging digunakan untuk membangun beberapa kandidat model yang kemudian akan dikombinasikan menjadi sebuah model final. Umumnya, model averaging diterapkan untuk memperoleh nilai prediksi suatu peubah respon.

```{r}
mod1 <- lm(Rings ~ ., data = df, na.action = na.fail)
mod2 <- dredge(global.model = mod1)
mod3 <- model.avg(mod2, delta < 4)
summary(mod3)
```

Berdasarkan hasil model averaging, model terbaik yang diperoleh adalah model dengan tujuh peubah penjelas, yaitu seluruh peubah penjelas kecuali peubah ```Length```. 

### Variable Selection
Variable selection digunakan untuk menyisihkan peubah penjelas yang tidak signifikan pada model regresi linier berganda. 

#### Regresi Linear
```{r}
reg1 <- lm(formula = Rings ~ ., data = df)
tidy(reg1) %>% 
  mutate(across(where(is.numeric), ~ format(round(.x,3), big.mark=",", scientific=F)))
```

#### Forward Stepwise Selection
Metode ini dimulai dengan model yang tidak mengandung prediktor dan kemudian menambahkan prediktor ke model, satu per satu, sampai semua prediktor berada dalam model. Pada setiap langkahnya, peubah yang memberikan peningkatan terbesar pada kebaikan model akan ditambahkan ke dalam model.

```{r}
stepAIC(reg1, direction = "forward")
```

Model yang dihasilkan melalui metode forward stepwise memiliki AIC sebesar 6323.75. 

#### Backward Stepwise Selection
Metode ini dimulai dengan model penuh yang berisi seluruh prediktor dan kemudian menghilangkan prediktor yang paling tidak berguna, satu per satu. 

```{r}
stepAIC(reg1, direction = "backward")
```
Model yang dihasilkan melalui metode backward stepwise memiliki AIC sebesar 6323.75. 

#### Hybrid Stepwise Selection
Metode ini merupakan gabungan dari metode forward stepwise selection dan backward stepwise selection.

```{r}
stepAIC(reg1, direction = "both")
```

Model yang dihasilkan melalui metode hybrid stepwise memiliki AIC sebesar 6323.75.

## Unsupervised Learning
### Hierarchical Clustering
Penggerombolan berhierarki dimulai dengan setiap satu amatan sebagai gerombolnya sendiri, kemudian terus mengelompokkan amatan-amatan ke dalam gerombol yang semakin besar. Dalam penggerombolan berhierarki, penentuan banyaknya gerombol dapat dilakukan menggunakan metode linkage (pautan), seperti complete linkage, average linkage, centroid linkage, dan single linkage.

#### Complete Linkage (Pautan Lengkap)
Pada complete linkage, jarak dua gerombol diukur dengan jarak terjauh antara sebuah objek dalam gerombol yang satu dengan sebuah objek dalam gerombol yang lain.

```{r}
fviz_nbclust(dt.scl, FUNcluster = hcut, method = "silhouette", hc_method = "complete", hc_metric="euclidean")
```

Berdasarkan metode complete linkage menggunakan jarak euclid, jumlah gerombol yang optimal adalah dua gerombol.

#### Average Linkage (Pautan Rataan)
Average linkage merupakan proses pengelompokkan yang didasarkan pada jarak rata-rata antar objeknya.

```{r}
fviz_nbclust(dt.scl, FUNcluster = hcut, method = "silhouette", hc_method = "average", hc_metric="euclidean")
```

Berdasarkan metode average linkage menggunakan jarak euclid, jumlah gerombol yang optimal adalah dua gerombol.

#### Centroid Linkage (Pautan Centroid)
Pada centroid linkage, jarak dua buah gerombol diukur sebagai jarak euclid antara kedua rataan (centroid) gerombol.

```{r}
fviz_nbclust(dt.scl, FUNcluster = hcut, method = "silhouette", hc_method = "centroid", hc_metric="euclidean")
```

Berdasarkan metode centroid linkage menggunakan jarak euclid, jumlah gerombol yang optimal adalah dua gerombol.

#### Single Linkage (Pautan Tunggal)
Pada single linkage, jarak dua gerombol diukur dengan jarak terdekat antara sebuah objek dalam gerombol yang satu dengan sebuah objek dalam gerombol yang lain.

```{r}
fviz_nbclust(dt.scl, FUNcluster = hcut, method = "silhouette", hc_method = "single", hc_metric="euclidean")
```

Berdasarkan metode single linkage menggunakan jarak euclid, jumlah gerombol yang optimal adalah dua gerombol.

Berdasarkan keempat metode pautan yang sudah dilakukan, jumlah gerombol yang akan dibentuk adalah dua gerombol. 

```{r}
hc.data <- eclust(dt, stand = TRUE, FUNcluster = "hclust", k=2, hc_method = "complete", hc_metric = "euclidean", graph = F)
hc.data$cluster #cluster dari setiap pengamatan
fviz_cluster(hc.data)
```

Setiap gerombol memiliki karakteristik sebagai berikut.

```{r}
aggregate(dt, by=list(cluster=hc.data$cluster), FUN = mean)
```

Interpretasi:

+ Unsur-unsur pada gerombol 1 dari yang paling mendominasi adalah Si, Na, Ca, Mg, Al, dan K.

+ Unsur-unsur pada gerombol 2 dari yang paling mendominasi adalah Si, Na, Ca, Al, Mg, dan K.

+ Kaca yang dibuat dengan unsur-unsur pada gerombol 1 lebih kuat daripada kaca yang dibuat dengan unsur-unsur pada gerombol 2.

### Non-hierarchical Clustering
Metode yang digunakan pada penggerombolan tak berhierarki adalah _K-Means clustering_. Metode ini merupakan pendekatan yang sederhana untuk membagi kumpulan data ke dalam K buah gerombol yang berbeda.

```{r}
fviz_nbclust(dt.scl, FUNcluster = kmeans, method = "wss")
```

Titik yang membentuk sudut ada pada nomor 3, sehingga jumlah gerombol yang akan dibentuk menggunakan metode k-means adalah tiga gerombol.

```{r}
kmeans.data <- eclust(dt, stand = TRUE, FUNcluster = "kmeans", k=3, graph = F)
fviz_cluster(kmeans.data)
```

Karakteristik yang dimiliki oleh setiap gerombol adalah sebagai berikut.

```{r}
kmeans.data$cluster
kmeans.data$centers
aggregate(dt, by=list(cluster=kmeans.data$cluster), FUN = mean)
```

Interpretasi:

+ Unsur-unsur pada gerombol 1 dari yang paling mendominasi adalah Si, Na, Ca, Mg, Al, dan K.

+ Unsur-unsur pada gerombol 2 dari yang paling mendominasi adalah Si, Na, Ca, Al, Mg, dan K.

+ Unsur-unsur pada gerombol 3 dari yang paling mendominasi adalah Si, Na, Ca, Al, Mg, dan K.

+ Kaca yang paling kuat secara berturut-turut adalah kaca yang dibuat dari unsur-unsur pada gerombol 2, gerombol 1, dan terakhir gerombol 3.